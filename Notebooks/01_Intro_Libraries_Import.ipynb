{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn_JjnfrnoIs"
      },
      "source": [
        "# Customer Segmentation on New York City AirBnB Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zT-kh2Kqlmf1"
      },
      "source": [
        "**Members:** <br>\n",
        "Tsoi Kwan Ma - 476914 <br>\n",
        "Łukasz Brzoska - 472892<br>\n",
        "Si-Tang Lin - 476912<br>\n",
        "I Putu Agastya Harta Pratama - 472876<br>\n",
        "Chi Phuong Dao - 474064<br>\n",
        "\n",
        "***Disclaimer on work distribution:*** <br>\n",
        "As most of the time we work simultaneously in a group, and each team members have different level understanding of coding and/or machine learning, making our responsibilities overlap between one-another."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu1qszooxT6T"
      },
      "source": [
        "## **Business Problem**\n",
        "With hundreds of thousands of listings available on the Airbnb platform across various locations, it is essential to understand customer preferences and create the appropriate segment effectively. This is crucial for both hosts and Airbnb to maximise occupancy rates, set optimal pricing, and offer personalised experiences.\n",
        "\n",
        "In this project, our team will an analyse Airbnb listings in New York City to optimise Airbnb's offerings and pricing strategies. By categorizing the listings based on price and room type, Airbnb can enhance its recommendation algorithms to better align with customer preferences.\n",
        "\n",
        "Later on, Machine Learning classification algorithms will be applied based on these variables:\n",
        "\n",
        "**Target Variable: customers segment (by price per room type) <br>\n",
        "Feature Variable: minimum nights, neighborhood group, hosts' listings count, number of review and the availablility of the listing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvf_ZUKAYzXF"
      },
      "source": [
        "## **Attribute Information**\n",
        "\n",
        "This dataset contains 48,895 rows and 16 columns.\n",
        "\n",
        "1. ID: A unique identifier for each listing.\n",
        "2. Name: The name of the Airbnb listing.\n",
        "3. Host_id: A unique identifier for each host.\n",
        "4. Host_name: The name of the host.\n",
        "5. Neighbourhood_group: The larger area or borough where the property is located (e.g., Manhattan, Brooklyn).\n",
        "6. Latitude: The latitude coordinate of the property’s location.\n",
        "7. Longitude: The longtitude coordinate of the property’s location.\n",
        "8. Room_type: The type of room offered, e.g., Entire home/apt, Private room, Shared room. <br>\n",
        "  *   Entire apartment - Studio apartment; a single apartment unit for yourself.\n",
        "  *   Private room - Shared room in an apartment.\n",
        "  *   Shared room - Dormitory situation.\n",
        "9. Price: The nightly price of the listing in USD.\n",
        "10. Minimum_nights: The minimum number of nights a guest must book.\n",
        "11. Calculated_host_listings_count: The total number of listings managed by a host.\n",
        "12. Availability_365: The number of days in a year the property is available for booking.\n",
        "13. Number_of_reviews: Total number of reviews received by the listing.\n",
        "14. Last_review: The date of the most recent review for the property.\n",
        "15. Reviews_per_month: The average number of reviews the property receives per month.\n",
        "\n",
        "With the type of data as follows: <br>\n",
        "\n",
        "*   Categorical: name, host_name neighbourhood_group, room_type.\n",
        "*   Numerical: id, host_id, latitude, longitude, price, minimum_nights, calculated_host_listings_count, availability_365, number_of_reviews, last_review, reviews_per_month."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH7_lh2CoNvu"
      },
      "source": [
        "## **Imports**\n",
        "Libraries and Data Imports is done in this section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf0mnz1g1xS1"
      },
      "source": [
        "### Libraries Imports\n",
        "*Run firstly after each new kernel session*\n",
        "\n",
        "All of the necessary libraries for our Machine Learning pipeline will be included within this section\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wGLy-CRjoH8G",
        "outputId": "2bcfacd2-e8dd-4075-b075-e14be6ce3a79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mapclassify in /usr/local/lib/python3.10/dist-packages (2.8.1)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from mapclassify) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from mapclassify) (1.26.4)\n",
            "Requirement already satisfied: pandas!=1.5.0,>=1.4 in /usr/local/lib/python3.10/dist-packages (from mapclassify) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from mapclassify) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from mapclassify) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.4->mapclassify) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.4->mapclassify) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.5.0,>=1.4->mapclassify) (2024.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->mapclassify) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->mapclassify) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas!=1.5.0,>=1.4->mapclassify) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "# Libraries Import for Data Preparation and Visualisation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "!pip install mapclassify\n",
        "from wordcloud import WordCloud\n",
        "from PIL import Image\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "import sqlite3\n",
        "import csv\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJnEjLahqQUr"
      },
      "outputs": [],
      "source": [
        "# AI is used to assist us with the code\n",
        "'''ChatGPT is used to help us check whether we have imported the necessary\n",
        "libraries to run, and test the model correctly.\n",
        "\n",
        "We also inquired AI for our syntax writing as well, since we are unsure about\n",
        "the exact naming of each libraries'''\n",
        "\n",
        "# Library for splitting the dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Libraries for Classification Algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# Library for model evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_c9vE-_gkxn"
      },
      "outputs": [],
      "source": [
        "# Library for encoding and normalisation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ahnVAE-lPgd"
      },
      "source": [
        "### Importing data\n",
        "Run firstly after each kernel session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8CokK-tJuq9l",
        "outputId": "6c7a2624-f788-4d05-8afe-243bfcef92ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-15 16:23:55--  https://drive.google.com/uc?id=18ZTZy0aOD63QZ7L0DGVwKoy1_kTIuWpL&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.181.113, 64.233.181.102, 64.233.181.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.181.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=18ZTZy0aOD63QZ7L0DGVwKoy1_kTIuWpL&export=download [following]\n",
            "--2025-01-15 16:23:55--  https://drive.usercontent.google.com/download?id=18ZTZy0aOD63QZ7L0DGVwKoy1_kTIuWpL&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 209.85.200.132, 2607:f8b0:4001:c08::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|209.85.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7077973 (6.8M) [application/octet-stream]\n",
            "Saving to: ‘AB_NYC_2019.csv’\n",
            "\n",
            "AB_NYC_2019.csv     100%[===================>]   6.75M  33.6MB/s    in 0.2s    \n",
            "\n",
            "2025-01-15 16:24:01 (33.6 MB/s) - ‘AB_NYC_2019.csv’ saved [7077973/7077973]\n",
            "\n",
            "--2025-01-15 16:24:01--  https://drive.google.com/uc?id=1uHykw8z1B0JQr0CywWpN4gwev4ETlAPZ&export=download\n",
            "Resolving drive.google.com (drive.google.com)... 64.233.181.113, 64.233.181.102, 64.233.181.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|64.233.181.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1uHykw8z1B0JQr0CywWpN4gwev4ETlAPZ&export=download [following]\n",
            "--2025-01-15 16:24:01--  https://drive.usercontent.google.com/download?id=1uHykw8z1B0JQr0CywWpN4gwev4ETlAPZ&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 209.85.200.132, 2607:f8b0:4001:c08::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|209.85.200.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7618560 (7.3M) [application/octet-stream]\n",
            "Saving to: ‘AB_NYC_2019.db’\n",
            "\n",
            "AB_NYC_2019.db      100%[===================>]   7.27M  33.5MB/s    in 0.2s    \n",
            "\n",
            "2025-01-15 16:24:04 (33.5 MB/s) - ‘AB_NYC_2019.db’ saved [7618560/7618560]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# AI is used to generate the code\n",
        "'''ChatGPT is used to assist us on streamlining the data import process.\n",
        "In hindsight, we want to be able to have the dataset ready each time we start\n",
        "a new Kernel session, without having to manually import every single time\n",
        "\n",
        "We are also insipred with this code from the previous Machine Learning Project\n",
        "of one of our team member'''\n",
        "\n",
        "# Dataset download\n",
        "file_id_dataset = '18ZTZy0aOD63QZ7L0DGVwKoy1_kTIuWpL' # AI generated\n",
        "download_url_dataset = f'https://drive.google.com/uc?id={file_id_dataset}&export=download' # AI generated\n",
        "# Database download\n",
        "file_id_db = '1uHykw8z1B0JQr0CywWpN4gwev4ETlAPZ'\n",
        "download_url_db = f'https://drive.google.com/uc?id={file_id_db}&export=download'\n",
        "\n",
        "# Download the file\n",
        "!wget -O AB_NYC_2019.csv '{download_url_dataset}' # AI generated\n",
        "!wget -O AB_NYC_2019.db '{download_url_db}'\n",
        "\n",
        "# Load the dataset\n",
        "df_nyc = pd.read_csv('AB_NYC_2019.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "K9WndxwntMga"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
